---
layout: single
title: " [CS 퀴즈] 오답노트 (7)"
categories: CS
tags:
  - CS
  - CS퀴즈
toc: true
toc_sticky: true
author_profile: false
sidebar:
---
# CS 퀴즈 (7)
## IPC(Inter Process Comunication)란?

프로세스는 완전히 독립된 공간을 가지고 있기 때문에, 다른 프로세스의 영향을 받지 않는다는 장점이 있다. 하지만, 서로 독립되어 있으므로 별도의 설비가 없이는 통신이 어렵다는 문제가 있다.   

이러한 문제를 극복하기 위해 커널 영역에서 IPC(Inter Process Comunication)라는 **내부 프로세스간 통신**을 제공하게되었다. 즉 **프로세스는 커널이 제공하는 IPC 설비를 이용해서 프로세스간 통신**을 할 수 있다.  

> **커널(Kernel)이란?**  
> 운영체제 자체도 소프트웨어이기 때문에 메모리에 올라가야 사용할 수 있다. 하지만 메모리 공간의 제약으로 운영체제 중 항상 필요한 부분만을 메모리에 올려놓는다. 이때 메모리에 상주하는 운영체제의 부분을 커널이라고 한다.  
> -> 그러므로 커널이란 **메모리에 상주하는 부분으로 운영체제의 핵심적인 부분** 이다.


프로세스 A가 현재 자원 a 를 B가 현재 자원 b를 가지고 있다고 가정할 시 B가 a 자원을 필요로한다면 이 때 운영체제가 전달을함  

이 때 직접통신(direct communication) 과 간접통신(indirect communication) 이 있다.   

**직접통신**은 프로세스 A가 a가 커널의 메세지 큐로 전달하고 커널이 그 자원 a를 프로세스 B에게 직접 전달한다.   

**간접통신**은 프로세스 A가 자원 a를 커널의 메세지 큐로 전달하고 나면 프로세스 B보고 직접와서 자원을 읽어 가라고 한다.  

이는 연산의 인터페이스에 대한 차이일뿐 실제 메세지 전송이 이루어지는 내부 구현은 커널의 중재에 의해 사실상 동일한 방식으로 이루어진다고 함.  

**직접통신** 의 경우 높은 커널 의존성을 가지게 되며 속도가 낮아지는 대신 운영체제가 직접 개입하다보니 **동기화 문제는 발생하지 않는다**

**간접통신** 중 공유 메모리 방식은 프로세스들이 주소 공간의 일부를 공유하는 방식이다. 각각의 프로세스는 독립적인 주소 공간을 가지고 있으므로 자신이 아닌 다른 프로세스의 메모리에 대한 접근은 불가능  

**하지만 운영체제는 공유메모리를 사용하는 시스템 콜을 지원해 서로 다른 프로세스들이 그들의 주소 공간 중 일부를 공유할 수 있도록 만든다.**  

중재자가 없어 곧바로 메모리에 접근할 수 있기 때문에 **빠르게 작동**할 수 있지만 서로 데이터에 일관성 문제가 발생할 수 있고 이는 **동기화문제가 발생한다는 뜻**.

---

# 📌 IPC 종류

### PIPE(파이프)

![](https://velog.velcdn.com/images/rlvy98/post/16b687c7-3ace-4061-a94f-479699a5c1bb/image.png)

- 파이프는 **두 개의 프로세스를 연결하고 하나의 프로세스는 데이터를 쓰기만 하고, 다른 프로세스는 데이터를 읽기만 가능**
- 부모 자식 간에 **단방향 통신**으로 자주 사용
- 한쪽 방향으로만 통신이 가능한 PIPE의 특징 때문에 Half-Duplex(반이중) 통신 이라고도 불림
- PIPE는 반이중 통신이기에 하나의 통신선로는 읽기/쓰기 중 하나만 가능하므로 만약 읽기/쓰기, 즉 송/수신을 모두 하기 원한다면 두 개의 파이프를 만들어야 가능

**장점**

- PIPE는 간단하게 사용 가능
- 한쪽 프로세스는 단지 읽기만하고 다른 프로세스는 단지 쓰기만 하는 단순한 데이터 흐름에 적합

**단점**

- Full-Duplex(전이중) 통신 방식으로 활용하려면 PIPE를 두 개 만들어야 하는데, 구현이 꽤나 복잡 (굳이 전이중을 활용해야한다면 PIPE말고 다른 방법을 찾는게 효율적)
- buffer가 상대적으로 작기때문에 overflow 될 높은 확률
- 부모 자식 관계의 프로세스들 사이에서 가능

### Named PIPE

![](https://velog.velcdn.com/images/rlvy98/post/93ffd22c-3dd9-4689-bf41-1c83de17b7b4/image.png)

- Named Pipe는 고유한 이름을 가지고 있으며, **파일 시스템에서 파일처럼 접근, 이러한 이름을 통해 여러 개의 프로세스가 특정 Named Pipe에 접근**하여 통신
- 보통 PIPE는 부모자식간에 사용하고 Named PIPE는 전혀 모르는 상태의 프로세스들 사이에서 통신할 경우 사용
- Named PIPE는 부모프로세스와 무관하게 **전혀 다른 모든 프로세스들 사이에서 통신이 가능**한데, 이유는 프로세스간에 통신을 위해 이름이 있는 파일을 매개체로 사용하기 때문

**장점**

- PIPE는 간단하게 사용 가능
- 한쪽 프로세스는 단지 읽기만하고 다른 프로세스는 단지 쓰기만 하는 단순한 데이터 흐름에 적합

**단점**

- 읽기/쓰기가 동시에 이루어지지 않음, 단방향 통신, read-only or write-only
- 기본 PIPE와 비슷

### Message Queue(메시지 큐)

![](https://velog.velcdn.com/images/rlvy98/post/b3fbb974-0ea5-4c68-891b-e4224c004d88/image.png)

- **FIFO(First-In First-Out, 선입선출) 자료구조**를 가지는 통신설비로 커널에서 관리, 입출력 방식으로보면 위에 Named PIPE와 동일
- **Message Queue는 메모리 공간**이라는 점, 어디서나 물건을 꺼낼 수 있는 컨테이너 벨트와 같다 보면 된다
- Message Queue에 쓸 데이터에 번호를 붙힘으로써 다수의 프로세스가 동시에 데이터를 쉽게 다룰 수 있다

**장점**

- 비동기 방식이기에 방대한 처리량이 있다면 큐에 넣은 후 나중에 처리 가능
- 다수의 프로세스들이 큐에 메시지를 수신, 발신 가능
- 분산처리 및 경쟁처리 방식에 사용

**단점**

- 메시지가 정말 잘 전달되었는지 알 수 없음
- 메시지와 메시지큐의 크기와 용량을 제대로 고려하지 않는다면 큐에 데이터를 넣고 나오는 과정에서 오버헤드가 발생
- 데이터가 많이 쌓일수록 추가적인 메모리 자원이 필요

### Shared Memory(공유 메모리)

![](https://velog.velcdn.com/images/rlvy98/post/1861bf9b-f9ac-4919-9972-5cb0d2f480f1/image.png)

- **공유메모리가 데이터 자체를 공유하도록 지원**하는 설비
- Shared Memory(공유 메모리)는 **프로세스간 메모리 영역을 공유해서 사용할 수 있도록 허용**
- 프로세스가 공유 메모리 할당을 커널에 요청하면 커널은 해당 프로세스에 메모리 공간을 할당해 주게되고, 이후 어떤 프로세스건 해당 메모리영역에 접근

**장점**

- 공유 메모리는 중개자 없이 곧바로 메모리에 접근할 수 있기 때문에 모든 IPC중에서 가장 빠르게 작동 (다시말해, 커널메모리영역에서 관리하기에 빠르게 접근 가능)
- 쉬운 데이터 공유: 공유 메모리를 사용하면 별도의 IPC 메커니즘을 구현하지 않아도 되기 때문에, 데이터 공유가 간단
- 시스템 자원 절약: 데이터 복사나 전송에 따른 오버헤드가 없기 때문에, 시스템 자원을 절약

**단점**

- 메시지 전달 방식이 아니기에 데이터를 읽어야하는 시점을 알 수 없다
- 커널 설정에 종속적이기에 사용하기전에 커널에서 허용하고 있는 공유메모리 사이즈에 따라 성능 편차가 발생
- 여러 프로세스가 동시에 공유 메모리에 접근할 수 있기 때문에, 데이터의 일관성을 유지하기 위해 동기화가 필요(디버깅 문제, 보안 문제..)

  

# 참고

- [https://velog.io/@yanghl98/OS운영체제-IPC란](https://velog.io/@yanghl98/OS%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-IPC%EB%9E%80)
- [https://yaelimeee.tistory.com/56](https://yaelimeee.tistory.com/56)
- [https://ikcoo.tistory.com/83](https://ikcoo.tistory.com/83)